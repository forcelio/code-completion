name: attn_lstm_vocab_1k
train:
    batch_size: 256
    is_load = False
    LOAD_EPOCH: 2
    epochs: 5
    num_workers: 6
    eval_period: 1
    checkpoint_period: 1
    device: cuda
    lr: 0.001
    lr_decay: 0.6
    clip_value: 5
model:
    hidden_size: 800
    embedding_sizeT: 512
    embedding_sizeN: 300
    dropout: 0.05
    num_layers: 1
    label_smoothing: 0
    pointer: False
    attn: True
data:
    truncate_size: 50
    N_filename: ./pickle_data/PY_non_terminal_small.pickle
    T_filename: ./pickle_data/PY_terminal_1k_whole.pickle
